{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 7655565,
     "sourceType": "datasetVersion",
     "datasetId": 4463361
    }
   ],
   "dockerImageVersionId": 30648,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Generate Embeddings\n",
    "---\n",
    "### This notebook is used to generate embeddings for a given folder with a specific structure\n",
    "\n",
    "The dataset should contain 2 sub-folders. For example: \n",
    "\n",
    "    i) dataset/anomaly\n",
    "    ii) dataset/non-anomaly\n",
    "\n",
    "**This code will take all the videos ending with .mp4 from both the folders and generate embeddings for each video**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Importing necessary Libraries\n",
    "\n",
    "To generate embeddings of a video, we need to first make a `Window` object. Then with that object we will create `WindowEmbedded` object. Using the object, we will get the embeddings. To do that we need these libraies:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from random import shuffle\n",
    "from glob import glob\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2024-02-19T13:30:42.942221Z",
     "iopub.execute_input": "2024-02-19T13:30:42.942563Z",
     "iopub.status.idle": "2024-02-19T13:30:42.948269Z",
     "shell.execute_reply.started": "2024-02-19T13:30:42.942537Z",
     "shell.execute_reply": "2024-02-19T13:30:42.946888Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Device Selection\n",
    "\n",
    "We will utilize GPU if possible. For M1 MacBooks, we will utilize Metal Accelarator `mps`. If run on any cuda supported machine, we will try to utilize `cuda`. If no accelarator found, we will set the device to `cpu`. Also, when cuda is selected, we print GPU properties using `torch` function."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "device = torch.device(\"mps\"\n",
    "                      if torch.backends.mps.is_available()\n",
    "                      else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "                      )\n",
    "\n",
    "# Select Device According to Availability\n",
    "print(\"Device selected:\", device)\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    !nvidia-smi\n",
    "    print()\n",
    "    print(\"Device type:\", device.type)\n",
    "    print(\"Capability:\", torch.cuda.get_device_capability(device))\n",
    "else:\n",
    "    print(\"Device capabilities are limited on MPSs and CPUs.\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-02-19T13:30:43.917207Z",
     "iopub.execute_input": "2024-02-19T13:30:43.917969Z",
     "iopub.status.idle": "2024-02-19T13:30:44.977992Z",
     "shell.execute_reply.started": "2024-02-19T13:30:43.917932Z",
     "shell.execute_reply": "2024-02-19T13:30:44.976815Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": "Device selected: cuda\nMon Feb 19 13:30:44 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  Tesla P100-PCIE-16GB           Off | 00000000:00:04.0 Off |                    0 |\n| N/A   39C    P0              27W / 250W |      2MiB / 16384MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|  No running processes found                                                           |\n+---------------------------------------------------------------------------------------+\n\nDevice type: cuda\nCapability: (6, 0)\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The Window Class\n",
    "\n",
    "This class takes a video path and return a `Window` object. A `Window` object is a generator object which can be used to iterate over the video frames in a sliding window fashion."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class Window:\n",
    "    def __init__(self, video_path,\n",
    "                 class_label_index,\n",
    "                 true_class_name='anomaly', \n",
    "                 window_size=4, \n",
    "                 stride=2,\n",
    "                 frame_average_count=5):\n",
    "        \"\"\"\n",
    "        A class to manage sliding windows over video frames.\n",
    "\n",
    "        :param video_path (str): The path to the video file.\n",
    "        :param window_size (int): The size of the sliding window (default is 4).\n",
    "        :param stride (int): The step size for moving the window (default is 2).\n",
    "        :param frame_average_count (int): The number of frames to be averaged for each group (default is 5).\n",
    "        \"\"\"\n",
    "        # The duration of the video in seconds\n",
    "        self.__video_duration = None\n",
    "        # The size of the sliding window\n",
    "        self.__window_size = window_size\n",
    "        # The step size for moving the window\n",
    "        self.__stride = stride\n",
    "        # The number of frames to be averaged for each group\n",
    "        self.__frame_average_count = frame_average_count\n",
    "        # The path to the video file\n",
    "        self.__video_path = video_path\n",
    "        # An array containing processed frames from the video\n",
    "        self.__total_frames = self.__get_frames()\n",
    "        # The average frame of the video\n",
    "        self.__averaged_frames = self.__get_average_frame()\n",
    "        # A sliding window of frames\n",
    "        self.__windows = self.__prepare_window()\n",
    "        # The index of the current window in the windowed clip\n",
    "        self.__window_index = 0\n",
    "        # Class label of the video path\n",
    "        self.class_label = 1 if video_path.split('/')[class_label_index] == true_class_name else 0\n",
    "\n",
    "    def next(self):\n",
    "        \"\"\"\n",
    "        This method strides to the next window in the windowed clip.\n",
    "        :return: numpy.ndarray: The next window in the windowed clip.\n",
    "        \"\"\"\n",
    "        if self.has_next():\n",
    "            self.__window_index += 1\n",
    "            return self.__windows[self.__window_index - 1]\n",
    "        else:\n",
    "            print(\"No next window\")\n",
    "            return None\n",
    "\n",
    "    def has_next(self):\n",
    "        \"\"\"\n",
    "        This method checks if there is a next window in the windowed clip.\n",
    "        :return: bool: True if there is a next window, False otherwise.\n",
    "        \"\"\"\n",
    "        return self.__window_index < len(self.__windows)\n",
    "\n",
    "    def current_window(self):\n",
    "        \"\"\"\n",
    "        This method returns the current window in the windowed clip.\n",
    "        :return: numpy.ndarray: The current window in the windowed clip.\n",
    "        \"\"\"\n",
    "        return self.__windows[self.__window_index]\n",
    "\n",
    "    def previous(self):\n",
    "        \"\"\"\n",
    "        This method strides to the previous window in the windowed clip.\n",
    "        :return: numpy.ndarray: The previous window in the windowed clip.\n",
    "        \"\"\"\n",
    "        if self.__window_index > 0:\n",
    "            self.__window_index -= 1\n",
    "            return self.__windows[self.__window_index]\n",
    "        else:\n",
    "            print(\"No previous window\")\n",
    "            return None\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        This method resets the window index to the beginning of the windowed clip.\n",
    "        \"\"\"\n",
    "        self.__window_index = 0\n",
    "\n",
    "    def __prepare_window(self):\n",
    "        \"\"\"\n",
    "        Prepares a sliding window of frames from the average frame of the video.\n",
    "\n",
    "        :returns:\n",
    "            numpy.ndarray: A sliding window of frames.\n",
    "\n",
    "        Notes:\n",
    "            This method divides the average frame into overlapping windows of 'window_size' frames.\n",
    "            The stride parameter determines the step size for moving the window.\n",
    "            The number of steps is calculated based on the average frame shape and video duration.\n",
    "            The resulting windows are stored in a numpy array.\n",
    "        \"\"\"\n",
    "        window = []\n",
    "        fps = self.__averaged_frames.shape[0] // self.__video_duration\n",
    "        steps = int(round((fps * self.__window_size) // self.__stride))\n",
    "\n",
    "        for i in range(0, len(self.__averaged_frames) - steps, steps):\n",
    "            window.append(self.__averaged_frames[i: i + steps * 2])\n",
    "        return np.array(window)\n",
    "\n",
    "    def __get_average_frame(self):\n",
    "        \"\"\"\n",
    "        Computes the average frame of the video by taking the mean of consecutive frames.\n",
    "\n",
    "        :returns:\n",
    "            numpy.ndarray: The average frame of the video.\n",
    "\n",
    "        Notes:\n",
    "            This method divides the video frames into groups, each containing 'avg_no' frames.\n",
    "            For each group, it computes the mean frame by averaging the pixel values of all frames in the group.\n",
    "        \"\"\"\n",
    "        reduced_frames = []\n",
    "        for i in range(0, len(self.__total_frames), self.__frame_average_count):\n",
    "            frames = self.__total_frames[i:i + self.__frame_average_count]\n",
    "            mean = np.mean(frames, axis=0)\n",
    "            reduced_frames.append(mean)\n",
    "        return np.array(reduced_frames)\n",
    "\n",
    "    def __get_frames(self):\n",
    "        \"\"\"\n",
    "        This method reads the video file and returns the frames\n",
    "\n",
    "        :return:\n",
    "            numpy.ndarray: An array containing processed frames from the video.\n",
    "        :raises:\n",
    "            IOError: If the video file cannot be read or does not exist.\n",
    "        \"\"\"\n",
    "        video = cv2.VideoCapture(self.__video_path)\n",
    "        if not video.isOpened():\n",
    "            raise IOError(\"Error reading video file\")\n",
    "\n",
    "        frame_count = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        fps = video.get(cv2.CAP_PROP_FPS)\n",
    "        self.__video_duration = frame_count // fps\n",
    "        frames = []\n",
    "        for i in range(0, frame_count):\n",
    "            video.set(1, i)\n",
    "            ret, frame = video.read()\n",
    "            if ret:\n",
    "                frame = cv2.resize(frame, (224, 224))\n",
    "                frame = frame.astype(\"float32\") / 255.0\n",
    "                frames.append(frame)\n",
    "        video.release()\n",
    "        return np.array(frames)\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        This method makes the object iterable.\n",
    "        :return: window: The object itself.\n",
    "        \"\"\"\n",
    "        return self  # Return self to make the object iterable\n",
    "\n",
    "    def __next__(self):\n",
    "        \"\"\"\n",
    "        This method returns the next window in the windowed clip. (used for iteration)\n",
    "        :return: numpy.ndarray: The next window in the windowed clip.\n",
    "        \"\"\"\n",
    "        if self.has_next():\n",
    "            self.__window_index += 1\n",
    "            return self.__windows[self.__window_index - 1]\n",
    "        else:\n",
    "            raise StopIteration\n",
    "\n",
    "    def get_current_window_stats(self):\n",
    "        \"\"\"\n",
    "        This method returns the stats of the current window.\n",
    "\n",
    "        :return:\n",
    "            dict: A dictionary containing the statistics of the current window.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"index\": self.__window_index,\n",
    "            \"frame_count\": len(self.__windows[self.__window_index]),\n",
    "            \"frame_shape\": self.__windows[self.__window_index].shape,\n",
    "            \"frame_dtype\": self.__windows[self.__window_index].dtype,\n",
    "        }\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (f\"Window (\\n\\tVideo Path = {self.__video_path}\\n\"\n",
    "                f\"\\tTotal Window = {self.__window_size}\\n\"\n",
    "                f\"\\tStride = {self.__stride}\\n\"\n",
    "                f\"\\tNo. Frames Averaged = {self.__frame_average_count}\\n\"\n",
    "                f\"\\tTotal Frames = {len(self.__total_frames)}\\n\"\n",
    "                f\"\\tAveraged Frames = {len(self.__averaged_frames)}\\n\"\n",
    "                f\"\\tShape = {self.__windows.shape}\\n)\")\n",
    "\n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self.__windows.shape"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-02-19T18:00:42.456298Z",
     "iopub.execute_input": "2024-02-19T18:00:42.456731Z",
     "iopub.status.idle": "2024-02-19T18:00:42.480709Z",
     "shell.execute_reply.started": "2024-02-19T18:00:42.456702Z",
     "shell.execute_reply": "2024-02-19T18:00:42.479838Z"
    },
    "trusted": true
   },
   "execution_count": 30,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The Embedding Model\n",
    "\n",
    "\n",
    "This is a simple CNN model for the embeddings generation. The model is a simple CNN model with 3 convolutional layers and 1 fully connected layer. The model is defined using the PyTorch library. The model is defined in the CNN class.\n",
    "\n",
    "The model is defined as follows:\n",
    "1. The first convolutional layer has 3 input channels, 32 output channels, 3 kernel size, 1 stride, and 0 padding.\n",
    "2. The second convolutional layer has 32 input channels, 64 output channels, 3 kernel size, 1 stride, and 0 padding.\n",
    "3. The third convolutional layer has 64 input channels, 128 output channels, 3 kernel size, 1 stride, and 0 padding.\n",
    "4. The pooling layer is a max pooling layer with 2 kernel size and 2 stride.\n",
    "5. The fully connected layer has 128 * 26 * 26 input features and 1024 output features.\n",
    "\n",
    "\n",
    "**NOTE:\n",
    "THIS IS A SIMPLE AND EXPERIMENTAL CNN MODEL FOR THE EMBEDDINGS. DON'T USE THIS MODEL FOR PRODUCTION. USE A BETTER MODEL FOR PRODUCTION**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class CNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple convolutional neural network (CNN) for generating embeddings from frames.\n",
    "\n",
    "    Methods:\n",
    "        forward(self, x): Defines the forward pass of the model.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, 1, 0, device=device)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1, 0, device=device)\n",
    "        self.d = nn.Dropout(0.5)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, 1, 0, device=device)\n",
    "        # Pooling layer, All are same\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        # Fully connected layer\n",
    "        self.fc1 = nn.Linear(128 * 26 * 26, 1024, device=device)  # Adjust input size based on your frame size\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the CNN.\n",
    "\n",
    "        Takes an input batch of images `x` and performs the following:\n",
    "            1. Converts the input to the device (e.g., GPU).\n",
    "            2. Applies ReLU activation to the output of the first convolutional layer.\n",
    "            3. Performs max pooling with a kernel size of 2.\n",
    "            4. Repeats steps 2 and 3 for the second and third convolutional layers.\n",
    "            5. Flattens the output of the last pooling layer.\n",
    "            6. Applies ReLU activation to the output of the fully-connected layer.\n",
    "            7. Returns the final feature vector.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input batch of images of shape (batch_size, channels, height, width).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output feature vector of shape (batch_size, 1024).\n",
    "        \"\"\"\n",
    "        x = x.to(device)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        # For batch size 1\n",
    "        x = x.view(-1, 128 * 26 * 26)\n",
    "        # For batch size > 1\n",
    "        # x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return x"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-02-19T18:00:44.850780Z",
     "iopub.execute_input": "2024-02-19T18:00:44.851139Z",
     "iopub.status.idle": "2024-02-19T18:00:44.861288Z",
     "shell.execute_reply.started": "2024-02-19T18:00:44.851110Z",
     "shell.execute_reply": "2024-02-19T18:00:44.860293Z"
    },
    "trusted": true
   },
   "execution_count": 31,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The WindowEmbedded Class\n",
    "\n",
    "This class takes a window object and returns the embeddings of the window using the CNN model. It maintains the sliding window of frames and extracts the embeddings of each frame using the CNN model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class WindowEmbedded:\n",
    "    def __init__(self, windows: Window):\n",
    "        self.__windows = windows\n",
    "        self.__embedding_model = CNN().to(device)\n",
    "        self.window_embeddings = self.__get_embeddings()\n",
    "        self.class_label = windows.class_label\n",
    "\n",
    "    def __get_embeddings(self):\n",
    "        embeddings_list = []\n",
    "        # for window in tqdm.tqdm(self.__windows, desc=\"Extracting Embeddings\", total=self.__windows.shape[0]):\n",
    "        for window in self.__windows:\n",
    "            frames = torch.tensor(window).permute(0, 3, 1, 2).to(device)\n",
    "            frame_embeddings_list = []\n",
    "            for frame in frames:\n",
    "                frame = frame.unsqueeze(0)\n",
    "                frame = frame.to(device)\n",
    "                frame_embeddings = self.__embedding_model(frame)\n",
    "                frame_embeddings_list.append(frame_embeddings.flatten().cpu().detach().numpy())\n",
    "            embeddings_list.append(frame_embeddings_list)\n",
    "        self.window_embeddings = np.array(embeddings_list)\n",
    "        return self.window_embeddings"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-02-19T13:30:46.795668Z",
     "iopub.execute_input": "2024-02-19T13:30:46.795984Z",
     "iopub.status.idle": "2024-02-19T13:30:46.803759Z",
     "shell.execute_reply.started": "2024-02-19T13:30:46.795959Z",
     "shell.execute_reply": "2024-02-19T13:30:46.802712Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating the Dataset\n",
    "\n",
    "In this function we create the dataset, which is basically 2 files. \n",
    "\n",
    "- `/kaggle/working/embeddings.npy`\n",
    "- `/kaggle/working/labels.npy`\n",
    "\n",
    "`/kaggle/working/embeddings.npy` contains all the embeddings of the video.\n",
    "`/kaggle/working/labels.npy` contains all the labels corresponding to the embeddings. We will feed these two files into our model to train.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def create_dataset(file_path: str,\n",
    "                   class_label_index: int,\n",
    "                   true_class_name: str = 'anomaly',\n",
    "                   shuffle_data: bool = True,\n",
    "                   video_ext: str = 'mp4',\n",
    "                   save: bool = True,\n",
    "                   checkpoints_count: int = 0):\n",
    "    \"\"\"\n",
    "    This function will take the folder path and class_label_index as input and will return the embeddings and\n",
    "    labels. We can also specify the true_class_name, shuffle_data and video_ext.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : str\n",
    "        The folder path containing the videos.\n",
    "    class_label_index : int\n",
    "        The index of the class name in the file path. For example, if the file path is\n",
    "        'dataset/anomaly/1.mp4', then the class_label_index will be 1. The index starts from 0.\n",
    "    true_class_name : str, optional\n",
    "        The true class name, where the class label is 1.\n",
    "        Default: 'anomaly'\n",
    "    shuffle_data : bool, optional\n",
    "        Whether to shuffle the data or not.\n",
    "        Default: True\n",
    "    video_ext : str, optional\n",
    "        The extension of the videos.\n",
    "        Default: 'mp4'\n",
    "    save : bool, optional\n",
    "        Whether to save the embeddings and labels or not.\n",
    "        Default: True\n",
    "    checkpoints_count : int, optional\n",
    "        The number of videos to save the embeddings and labels after. 0 means No checkpoints.\n",
    "        Default: 0\n",
    "\n",
    "    Notes\n",
    "    ----------\n",
    "    The embeddings will be of shape [x, 4, 24, 1024] and labels will be of shape [x]. The labels will be 0 for\n",
    "    non-anomaly and 1 for anomaly. The embeddings and labels will be saved in the same folder with the name\n",
    "    embeddings.npy and labels.npy. Some videos may have less than 4 frames or 24 features. This problem is solved by\n",
    "    padding the videos with zeros to make them of the same shape using np.pad.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    all_embeddings : np.ndarray\n",
    "        The embeddings and labels of the videos.\n",
    "    all_labels : np.ndarray\n",
    "        The labels of the videos.\n",
    "    \"\"\"\n",
    "    # Create glob path for the videos\n",
    "    rgx = file_path + f'/*/*.{video_ext}'\n",
    "    # extract the paths of the videos\n",
    "    paths = glob(rgx)\n",
    "    # Randomize the order of the videos\n",
    "    if shuffle_data:\n",
    "        shuffle(paths)\n",
    "\n",
    "    # Embeddings and Labels\n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Variables for checkpoint\n",
    "    # Save the embeddings and labels after every 100 videos\n",
    "    count = 0\n",
    "    checkpoints = 0\n",
    "    # Iterate over the paths and extract the embeddings\n",
    "    for video_path in tqdm(paths, desc=\"Extracting Embeddings\"):\n",
    "        if checkpoints_count != 0:\n",
    "            if count == checkpoints:\n",
    "                # create checkpoint\n",
    "                np.save(f'embeddings_{checkpoints}.npy', all_embeddings)\n",
    "                np.save(f'labels_{checkpoints}.npy', all_labels)\n",
    "                checkpoints += 1\n",
    "                count = 0\n",
    "        try:\n",
    "            window = Window(video_path, class_label_index, true_class_name=true_class_name)\n",
    "            window_embed_object = WindowEmbedded(window)\n",
    "            embeddings = window_embed_object.window_embeddings\n",
    "            # Try to append the embeddings and labels\n",
    "            all_embeddings.append(embeddings)\n",
    "            all_labels.append(window.class_label)\n",
    "            count += 1\n",
    "        except ValueError:\n",
    "            # print error in red\n",
    "            print(f\"\\n\\033[91mError windowing video: {video_path}\\033[0m\")\n",
    "            continue\n",
    "\n",
    "    # Validate the embeddings and labels\n",
    "    assert len(all_embeddings) == len(all_labels)\n",
    "    \n",
    "    \"\"\"\n",
    "    Now we have created the list of embeddings and the list of labels. But, there might be cases where the number of\n",
    "    windows, frames, or features in the embeddings is not the same for all the embeddings. Normally the shape should\n",
    "    be [x, 4, 24, 1024], where x is the number of videos.\n",
    "    \n",
    "    To solve this problem, we will pad the embeddings with 0 where the number of windows, frames, or features is less\n",
    "    than the maximum. This will make all the embeddings of the same shape.\n",
    "    \n",
    "    We will use np.pad to pad the embeddings with 0. keras.preprocessing.sequence.pad_sequences can also be used but\n",
    "    it is slower than np.pad and also it is not recommended for 3D arrays.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Pad the embeddings and labels\n",
    "    max_windows = max([len(embeddings) for embeddings in all_embeddings])\n",
    "    max_frames = max([len(embeddings[0]) for embeddings in all_embeddings])\n",
    "    max_features = max([len(embeddings[0][0]) for embeddings in all_embeddings])\n",
    "\n",
    "    # Pad the embeddings with 0 where the number of windows, frames, or features is less than the maximum\n",
    "    padded_embeddings = []\n",
    "    for embedding in tqdm(all_embeddings, desc=\"Padding Embeddings where necessary\"):\n",
    "        if embedding.shape[0] < max_windows or embedding.shape[1] < max_frames or embedding.shape[2] < max_features:\n",
    "            pad_widths = [(0, max_windows - embedding.shape[0]),\n",
    "                          (0, max_frames - embedding.shape[1]),\n",
    "                          (0, max_features - embedding.shape[2])]\n",
    "            padded_embedding = np.pad(embedding, pad_widths, mode='constant', constant_values=0.0)\n",
    "            padded_embeddings.append(padded_embedding)\n",
    "        else:\n",
    "            padded_embeddings.append(embedding)\n",
    "\n",
    "    padded_arrays = np.asarray(padded_embeddings, dtype='float32')\n",
    "\n",
    "    # Save the embeddings and labels and return\n",
    "    all_embeddings = np.array(padded_arrays)\n",
    "    all_labels = np.array(all_labels)\n",
    "    if save:\n",
    "        np.save('embeddings.npy', all_embeddings)\n",
    "        np.save('labels.npy', all_labels)\n",
    "    return all_embeddings, all_labels"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-02-19T16:26:26.823154Z",
     "iopub.execute_input": "2024-02-19T16:26:26.823531Z",
     "iopub.status.idle": "2024-02-19T16:26:26.838046Z",
     "shell.execute_reply.started": "2024-02-19T16:26:26.823504Z",
     "shell.execute_reply": "2024-02-19T16:26:26.837043Z"
    },
    "trusted": true
   },
   "execution_count": 27,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### This cell will initiate the dataset creation process"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "embeddings, labels = create_dataset(\n",
    "    '/kaggle/input/fydp-dataset-v1/data',\n",
    "    class_label_index=5,\n",
    "    true_class_name='anomaly'\n",
    ")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-02-19T16:26:27.666569Z",
     "iopub.execute_input": "2024-02-19T16:26:27.666925Z",
     "iopub.status.idle": "2024-02-19T17:47:41.180489Z",
     "shell.execute_reply.started": "2024-02-19T16:26:27.666896Z",
     "shell.execute_reply": "2024-02-19T17:47:41.179611Z"
    },
    "trusted": true
   },
   "execution_count": 28,
   "outputs": [
    {
     "name": "stderr",
     "text": "Extracting Embeddings:  10%|█         | 369/3655 [07:38<1:06:31,  1.21s/it]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "\n\u001B[91mError windowing video: /kaggle/input/fydp-dataset-v1/data/anomaly/714_p2_Arson007_x264_016.mp4\u001B[0m\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "Extracting Embeddings:  15%|█▍        | 541/3655 [11:19<1:09:38,  1.34s/it]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "\n\u001B[91mError windowing video: /kaggle/input/fydp-dataset-v1/data/nonanomaly/Normal_Videos581_x264_008.mp4\u001B[0m\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "Extracting Embeddings:  19%|█▉        | 692/3655 [14:38<1:05:03,  1.32s/it]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "\n\u001B[91mError windowing video: /kaggle/input/fydp-dataset-v1/data/nonanomaly/Normal_Videos039_x264_005.mp4\u001B[0m\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "Extracting Embeddings:  33%|███▎      | 1204/3655 [26:07<52:03,  1.27s/it]  ",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "\n\u001B[91mError windowing video: /kaggle/input/fydp-dataset-v1/data/anomaly/721_p2_Arson011_x264_000.mp4\u001B[0m\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "Extracting Embeddings:  40%|███▉      | 1453/3655 [31:44<46:46,  1.27s/it]  ",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "\n\u001B[91mError windowing video: /kaggle/input/fydp-dataset-v1/data/anomaly/854_p3_Arson013_x264_006.mp4\u001B[0m\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "Extracting Embeddings:  51%|█████     | 1868/3655 [41:03<36:32,  1.23s/it]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "\n\u001B[91mError windowing video: /kaggle/input/fydp-dataset-v1/data/anomaly/74_Abuse004_x264_055.mp4\u001B[0m\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "Extracting Embeddings:  52%|█████▏    | 1910/3655 [42:00<37:00,  1.27s/it]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "\n\u001B[91mError windowing video: /kaggle/input/fydp-dataset-v1/data/anomaly/33_Arson016_x264_005.mp4\u001B[0m\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "Extracting Embeddings:  54%|█████▍    | 1987/3655 [43:45<35:02,  1.26s/it]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "\n\u001B[91mError windowing video: /kaggle/input/fydp-dataset-v1/data/anomaly/Fighting002_x264_008.mp4\u001B[0m\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "Extracting Embeddings:  55%|█████▍    | 1993/3655 [43:53<34:19,  1.24s/it]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "\n\u001B[91mError windowing video: /kaggle/input/fydp-dataset-v1/data/anomaly/RoadAccidents028_x264_001.mp4\u001B[0m\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "Extracting Embeddings:  55%|█████▌    | 2027/3655 [44:38<33:49,  1.25s/it]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "\n\u001B[91mError windowing video: /kaggle/input/fydp-dataset-v1/data/nonanomaly/Normal_Videos140_x264_005.mp4\u001B[0m\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "Extracting Embeddings:  61%|██████    | 2237/3655 [49:22<30:40,  1.30s/it]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "\n\u001B[91mError windowing video: /kaggle/input/fydp-dataset-v1/data/anomaly/785_p2_Arson026_x264_018.mp4\u001B[0m\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "Extracting Embeddings:  71%|███████   | 2578/3655 [57:01<22:32,  1.26s/it]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "\n\u001B[91mError windowing video: /kaggle/input/fydp-dataset-v1/data/anomaly/Shooting046_x264_016.mp4\u001B[0m\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "Extracting Embeddings:  75%|███████▍  | 2724/3655 [1:00:16<19:24,  1.25s/it]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "\n\u001B[91mError windowing video: /kaggle/input/fydp-dataset-v1/data/nonanomaly/890_p4_Arson013_x264_001.mp4\u001B[0m\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "Extracting Embeddings:  78%|███████▊  | 2847/3655 [1:03:02<16:42,  1.24s/it]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "\n\u001B[91mError windowing video: /kaggle/input/fydp-dataset-v1/data/anomaly/865_p3_Arson031_x264_003.mp4\u001B[0m\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "Extracting Embeddings:  82%|████████▏ | 2989/3655 [1:06:11<13:21,  1.20s/it]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "\n\u001B[91mError windowing video: /kaggle/input/fydp-dataset-v1/data/anomaly/155_p14_Abuse011_x264_001.mp4\u001B[0m\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "Extracting Embeddings:  84%|████████▍ | 3079/3655 [1:08:12<13:06,  1.37s/it]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "\n\u001B[91mError windowing video: /kaggle/input/fydp-dataset-v1/data/anomaly/RoadAccidents033_x264_001.mp4\u001B[0m\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "Extracting Embeddings:  94%|█████████▎| 3422/3655 [1:15:52<05:07,  1.32s/it]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "\n\u001B[91mError windowing video: /kaggle/input/fydp-dataset-v1/data/nonanomaly/Normal_Videos061_x264_058.mp4\u001B[0m\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "Extracting Embeddings:  94%|█████████▍| 3448/3655 [1:16:27<04:14,  1.23s/it]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "\n\u001B[91mError windowing video: /kaggle/input/fydp-dataset-v1/data/nonanomaly/623_Abuse045_x264_016.mp4\u001B[0m\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "Extracting Embeddings:  97%|█████████▋| 3545/3655 [1:18:39<02:28,  1.35s/it]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "\n\u001B[91mError windowing video: /kaggle/input/fydp-dataset-v1/data/anomaly/306_p1_Abuse025_x264_000.mp4\u001B[0m\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "Extracting Embeddings: 100%|██████████| 3655/3655 [1:21:11<00:00,  1.33s/it]\nPadding Embeddings where necessary: 100%|██████████| 3636/3636 [00:00<00:00, 475522.73it/s]\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Checking the shapes\n",
    "\n",
    "This cell will verify if the two npy files are OK"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"Video Embeddings Shape: \", embeddings.shape)\n",
    "print(\"Video Labels Shape: \", labels.shape)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-02-19T18:08:38.467619Z",
     "iopub.execute_input": "2024-02-19T18:08:38.467943Z",
     "iopub.status.idle": "2024-02-19T18:08:38.473223Z",
     "shell.execute_reply.started": "2024-02-19T18:08:38.467919Z",
     "shell.execute_reply": "2024-02-19T18:08:38.472305Z"
    },
    "trusted": true
   },
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "text": "Video Embeddings Shape:  (3636, 4, 24, 1024)\nVideo Labels Shape:  (3636,)\n",
     "output_type": "stream"
    }
   ]
  }
 ]
}
